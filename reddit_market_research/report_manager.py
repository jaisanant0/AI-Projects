from datetime import datetime
import logging
from typing import List, Dict
from json_schemas import PainPoint, ResearchState
import matplotlib.pyplot as plt
import pandas as pd
import os
from wordcloud import WordCloud
from vector_manager import VectorDBManager
import markdown
import base64
from weasyprint import HTML, CSS
import random
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class ReportGenerator:
    """Generates a report of the research"""

    def __init__(self, project_id: str, projects_path: str, vector_db: VectorDBManager):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.projects_path = projects_path  
        self.project_id = project_id
        self.visualizations_path = os.path.join(projects_path, str(self.project_id), "visualizations") 
        os.makedirs(self.visualizations_path, exist_ok=True) 

        self.vector_db = vector_db
        
    def generate_visualizations(self) -> Dict[str, str]:
        """Generate visualizations for the report"""

        unique_pain_points = self.vector_db.get_unique_pain_points(
            project_id=self.project_id
        )
        logger.info(f"Found {len(unique_pain_points)} unique pain points")

        plt.style.use('seaborn-v0_8')
        viz_files = {}

        categories = [pp["category"] for pp in unique_pain_points]
        category_counts = pd.Series(categories).value_counts()

        fig, ax = plt.subplots(figsize=(10, 8))
        ax.pie(category_counts.values, 
               labels=category_counts.index, 
               autopct='%1.1f%%')
        ax.set_title('Pain Points by Category')
        viz_files['categories'] = os.path.join(self.visualizations_path, f'pain_points_categories_{self.timestamp}.png')
        plt.savefig(viz_files['categories'], dpi=300, bbox_inches='tight')
        plt.close()

        # Word cloud
        all_text = ' '.join([pp["content"] for pp in unique_pain_points])
        wordcloud = WordCloud(
            width=800, 
            height=400, 
            max_words=100,
            background_color='white').generate(all_text)
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title('Pain Points Word Cloud')
        viz_files['wordcloud'] = os.path.join(self.visualizations_path, f'wordcloud_{self.timestamp}.png')
        plt.savefig(viz_files['wordcloud'], dpi=300, bbox_inches='tight')
        plt.close()

        return viz_files

    def generate_markdown_report(self, state: ResearchState, viz_files: Dict[str, str]) -> str:
        """Generate a markdown report of the research"""
        unique_pain_points = self.vector_db.get_unique_pain_points(
            project_id=self.project_id
        )

        report = f"""# Reddit Market Research Report

## Project Overview
**Project Idea:** {state.project_idea}
**Generated On:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Keywords:** {state.keywords}
**Total Posts Analyzed:** {len(state.reddit_posts)}
**Total Comments Analyzed:** {len(state.reddit_comments)}
**Pain Points Identified:** {len(unique_pain_points)}

## Executive Summary
This report presents a comprehensive analysis of Reddit discussions related to "{state.project_idea}".
We analyzed {len(state.reddit_posts)} posts and {len(state.reddit_comments)} comments to identify {len(unique_pain_points)} unique pain points.

## Key Findings

### Top Pain Points by Category
"""

        categories = {}
        for pp in unique_pain_points:
            category = pp["category"]
            categories[category] = categories.get(category, 0) + 1
        for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            report += f"- **{category}**: {count} pain points\n"
        
        report += f"""
## Detailed Pain Points Analysis

### Top 10 Pain Points
"""

        #get reandom for now 
        random_pain_points = random.sample(unique_pain_points, 10) if len(unique_pain_points) > 10 else unique_pain_points
        for i, pp in enumerate(random_pain_points, 1):
            report += f"""
#### {i}. {pp["category"]} Issue
**Content:** {pp["content"][:500]}...
"""

        report += """
## Visualizations

### Pain Points by Category
![Pain Points Categories](pain_points_categories.png)

### Word Cloud
![Word Cloud](wordcloud.png)

---
*This report was generated by Reddit Market Research AI Agent*
"""

        return report
    
    def generate_markdown(self, state: ResearchState) -> str:
        """Generate a markdown report of the research from LLM"""
        unique_pain_points = self.vector_db.get_unique_pain_points(
            project_id=state.project_id
        )

        report = f"""# Reddit Market Research Report 
## Project Overview
**Project Idea:** {state.project_idea}
**Generated On:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Keywords:** {", ".join(state.keywords)}
**Total Posts Analyzed:** {len(state.reddit_posts)}
**Total Comments Analyzed:** {len(state.reddit_comments)}
**Pain Points Identified:** {len(unique_pain_points)}

## Executive Summary
This report presents a comprehensive analysis of Reddit discussions related to "{state.project_idea}".
We analyzed {len(state.reddit_posts)} posts and {len(state.reddit_comments)} comments to identify {len(unique_pain_points)} unique pain points. 

## Summary of Pain Points 
""" 
        summarized_pain_points = state.summarized_pain_points.summarized_pain_points
        for pp in summarized_pain_points: 
            theme_name = pp.theme_name
            description = pp.description
            report += f"- **{theme_name}**: {description}\n"
        
        report += """
## Key Insight
"""
        insights = state.summarized_pain_points.key_insights.insight
        report += f"- {insights}\n"

        report += """
## Solutions
"""
        solutions = state.summarized_llm_solutions.summarized_llm_solutions
        for s in solutions:
            theme_name = s.theme_name
            description = s.solution
            report += f"- **{theme_name}**: {description}\n"

        report += """
## Visualizations

### Pain Points by Category
![Pain Points Categories](pain_points_categories.png)

### Word Cloud
![Word Cloud](wordcloud.png)

---
*This report was generated by Reddit Market Research AI Agent*
"""
        return report 

    def generate_pdf_report(self, markdown_content: str, viz_files: Dict[str, str]) -> str:
        """Convert markdown to PDF"""

        # Convert markdown to HTML with extensions for better formatting
        html_content = markdown.markdown(
            markdown_content, 
            extensions=['tables', 'fenced_code', 'nl2br']
        )

        logger.info(f"Original HTML content length: {len(html_content)}")
        logger.info(f"Viz files available: {list(viz_files.keys())}")

        # Helper function to convert image to base64
        def image_to_base64(image_path: str) -> str:
            """Convert image file to base64 data URI"""
            try:
                logger.info(f"Converting image to base64: {image_path}")
                if not os.path.exists(image_path):
                    logger.error(f"Image file does not exist: {image_path}")
                    return ""
                
                with open(image_path, 'rb') as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                    data_uri = f"data:image/png;base64,{encoded_string}"
                    logger.info(f"Successfully converted image to base64, length: {len(data_uri)}")
                    return data_uri
            except Exception as e:
                logger.error(f"Error converting image to base64: {e}")
                return ""

        # Replace image references with base64 encoded images for better PDF rendering
        if 'categories' in viz_files:
            logger.info(f"Processing categories image: {viz_files['categories']}")
            if os.path.exists(viz_files['categories']):
                categories_base64 = image_to_base64(viz_files['categories'])
                if categories_base64:
                    old_content = html_content
                    html_content = html_content.replace(
                        '![Pain Points Categories](pain_points_categories.png)',
                        f'<div style="text-align: center; margin: 20px 0;"><img src="{categories_base64}" alt="Pain Points Categories" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 5px;"></div>'
                    )
                    if old_content != html_content:
                        logger.info("Successfully replaced categories image in HTML")
                    else:
                        logger.warning("Categories image replacement did not occur - markdown pattern not found")
                        # Try alternative patterns
                        html_content = html_content.replace(
                            '<p><img alt="Pain Points Categories" src="pain_points_categories.png" /></p>',
                            f'<div style="text-align: center; margin: 20px 0;"><img src="{categories_base64}" alt="Pain Points Categories" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 5px;"></div>'
                        )
                        html_content = html_content.replace(
                            '<img alt="Pain Points Categories" src="pain_points_categories.png" />',
                            f'<img src="{categories_base64}" alt="Pain Points Categories" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 5px;">'
                        )
                else:
                    logger.error("Failed to convert categories image to base64")
            else:
                logger.error(f"Categories image file does not exist: {viz_files['categories']}")
        
        if 'wordcloud' in viz_files:
            logger.info(f"Processing wordcloud image: {viz_files['wordcloud']}")
            if os.path.exists(viz_files['wordcloud']):
                wordcloud_base64 = image_to_base64(viz_files['wordcloud'])
                if wordcloud_base64:
                    old_content = html_content
                    html_content = html_content.replace(
                        '![Word Cloud](wordcloud.png)',
                        f'<div style="text-align: center; margin: 20px 0;"><img src="{wordcloud_base64}" alt="Word Cloud" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 5px;"></div>'
                    )
                    if old_content != html_content:
                        logger.info("Successfully replaced wordcloud image in HTML")
                    else:
                        logger.warning("Wordcloud image replacement did not occur - markdown pattern not found")
                        # Try alternative patterns
                        html_content = html_content.replace(
                            '<p><img alt="Word Cloud" src="wordcloud.png" /></p>',
                            f'<div style="text-align: center; margin: 20px 0;"><img src="{wordcloud_base64}" alt="Word Cloud" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 5px;"></div>'
                        )
                        html_content = html_content.replace(
                            '<img alt="Word Cloud" src="wordcloud.png" />',
                            f'<img src="{wordcloud_base64}" alt="Word Cloud" style="max-width: 90%; height: auto; border: 1px solid #ddd; border-radius: 5px;">'
                        )
                else:
                    logger.error("Failed to convert wordcloud image to base64")
            else:
                logger.error(f"Wordcloud image file does not exist: {viz_files['wordcloud']}")

        logger.info(f"Final HTML content length: {len(html_content)}")
        
        css_content = """
        <style>
        @page {
            size: A4;
            margin: 2cm;
        }
        body { 
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
            line-height: 1.6;
            color: #333;
            max-width: 100%;
        }
        h1 { 
            color: #2c3e50; 
            border-bottom: 3px solid #3498db; 
            padding-bottom: 10px;
            margin-top: 30px;
            margin-bottom: 20px;
        }
        h2 { 
            color: #34495e; 
            border-bottom: 2px solid #bdc3c7; 
            padding-bottom: 5px;
            margin-top: 25px;
            margin-bottom: 15px;
        }
        h3 { 
            color: #7f8c8d; 
            margin-top: 20px;
            margin-bottom: 10px;
        }
        h4 {
            color: #5d6d7e;
            margin-top: 15px;
            margin-bottom: 8px;
        }
        p {
            margin-bottom: 12px;
            text-align: justify;
        }
        ul, ol {
            margin-bottom: 15px;
            padding-left: 25px;
        }
        li {
            margin-bottom: 5px;
        }
        strong {
            color: #2c3e50;
        }
        .metric { 
            background: #ecf0f1; 
            padding: 15px; 
            margin: 15px 0; 
            border-radius: 5px;
            border-left: 4px solid #3498db;
        }
        .pain-point { 
            background: #fff3cd; 
            padding: 15px; 
            margin: 15px 0; 
            border-left: 4px solid #ffc107;
            border-radius: 5px;
        }
        img { 
            max-width: 90% !important; 
            height: auto !important; 
            margin: 20px auto !important;
            display: block !important;
            border: 1px solid #ddd;
            border-radius: 5px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        div {
            page-break-inside: avoid;
        }
        hr {
            border: none;
            border-top: 2px solid #bdc3c7;
            margin: 30px 0;
        }
        </style>
        """

        full_html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>Reddit Research Report</title>
            {css_content}
        </head>
        <body>
            {html_content}
        </body>
        </html>
        """

        base_path = os.path.join(self.projects_path, str(self.project_id))
        pdf_filename = os.path.join(base_path, f"reddit_research_report_{self.timestamp}.pdf")
        
        try:
            # Use weasyprint options for better rendering
            HTML(string=full_html, base_url=self.projects_path).write_pdf(
                pdf_filename,
                stylesheets=None,
                optimize_images=True
            )
            logger.info(f"PDF report generated successfully: {pdf_filename}")
        except Exception as e:
            logger.error(f"Error generating PDF: {e}")
            raise

        return pdf_filename

if __name__ == "__main__": 
    report_generator = ReportGenerator(projects_path="projects") 
    pain_points = [
        PainPoint(
            id="1",
            content="I'm having trouble finding a good job",
            category="Job Search",
            sources_posts=["123", "456", "789"]
        )
    ] 
    report_generator.generate_visualizations(pain_points)